# -----
# This YAML describes the ML Inference workflow for TLess in Knative.
#
# See here for more details on the workflow:
# https://github.com/faasm/experiment-tless/blob/main/workflows/ml-inference/README.md
# -----
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: ingress-to-partition
  namespace: tless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: ingress-to-load
  namespace: tless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: pre-inf-to-predict
  namespace: tless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: ml-inference-partition
  namespace: tless
spec:
  template:
    spec:
      runtimeClassName: ${RUNTIME_CLASS_NAME}
      containers:
        - image: ghcr.io/coco-serverless/tless-knative-worker:${TLESS_VERSION}
          ports:
            - containerPort: 8080
          workingDir: /workflows/ml-inference/knative
          command: [ "./target/release/tless-cloudevent-handler" ]
          env:
            - name: TLESS_MODE
              value: "${TLESS_MODE}"
    metadata:
      labels:
        tless.workflows/name: ml-inference-partition
      annotations:
        # NOTE: we may have to enable this annotation in Kata's config file
        # under hypervisor.qemu.enable_annotations (add 'default_memory')
        io.katacontainers.config.hypervisor.default_memory: "6144"
---
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: ml-inference-load
  namespace: tless
spec:
  template:
    spec:
      runtimeClassName: ${RUNTIME_CLASS_NAME}
      containers:
        - image: ghcr.io/coco-serverless/tless-knative-worker:${TLESS_VERSION}
          ports:
            - containerPort: 8080
          workingDir: /workflows/ml-inference/knative
          command: [ "./target/release/tless-cloudevent-handler" ]
          env:
            - name: TLESS_MODE
              value: "${TLESS_MODE}"
    metadata:
      labels:
        tless.workflows/name: ml-inference-load
      annotations:
        # NOTE: we may have to enable this annotation in Kata's config file
        # under hypervisor.qemu.enable_annotations (add 'default_memory')
        io.katacontainers.config.hypervisor.default_memory: "6144"
---
apiVersion: sinks.knative.dev/v1alpha1
kind: JobSink
metadata:
  name: ml-inference-predict
  namespace: tless
spec:
  job:
    spec:
      completions: 1
      parallelism: 1
      backoffLimit: 4
      # Clean-up jobs once they are finished
      ttlSecondsAfterFinished: 30
      template:
        spec:
          runtimeClassName: ${RUNTIME_CLASS_NAME}
            # restartPolicy: OnFailure
          restartPolicy: Never
          containers:
            - name: main
              # TODO: pin to version
              # image: ghcr.io/coco-serverless/tless-knative-worker:${TLESS_VERSION}
              image: ghcr.io/coco-serverless/tless-knative-worker@sha256:9e5d55e4879aa97092e7a25d5e6e7d1c9b7979f45244060003ba0943bc7bad6b
              ports:
                - containerPort: 8080
              workingDir: /workflows/ml-inference/knative
              command: [ "./target/release/tless-cloudevent-handler" ]
              env:
                - name: CE_FROM_FILE
                  value: "on"
                - name: TLESS_MODE
                  value: "${TLESS_MODE}"
        metadata:
          labels:
            tless.workflows/name: ml-inference-predict
          annotations:
            # NOTE: we may have to enable this annotation in Kata's config file
            # under hypervisor.qemu.enable_annotations (add 'default_memory')
            io.katacontainers.config.hypervisor.default_memory: "6144"
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-one-subscription
  namespace: tless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: ingress-to-partition
  reply:
    ref:
      apiVersion: messaging.knative.dev/v1
      kind: InMemoryChannel
      name: pre-inf-to-predict
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: ml-inference-partition
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-two-subscription
  namespace: tless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: ingress-to-load
  reply:
    ref:
      apiVersion: messaging.knative.dev/v1
      kind: InMemoryChannel
      name: pre-inf-to-predict
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: ml-inference-load
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-three-subscription
  namespace: tless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: pre-inf-to-predict
  subscriber:
    ref:
      apiVersion: sinks.knative.dev/v1alpha1
      kind: JobSink
      name: ml-inference-predict
---
