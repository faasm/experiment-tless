# -----
# This YAML describes the word-count workflow for TLess in Knative.
#
# This workflow describes a fan-out, fan-in, pattern. The driver service
# fans-out, instantiatating many mapper functions. Then the reducer function
# waits for all mapper functions to finish, and aggreagates the results.
# -----
# We need to have as many channels as edges in our workflow DAG. Alternatively,
# we could enforce edges by using a Broker/Trigger pattern and filtering on
# CloudEvent properties
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: ingress-to-splitter
  namespace: tless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: splitter-to-mapper
  namespace: tless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
apiVersion: messaging.knative.dev/v1
kind: Channel
metadata:
  name: mapper-to-reducer
  namespace: tless
spec:
  channelTemplate:
    apiVersion: messaging.knative.dev/v1
    kind: InMemoryChannel
---
# We can re-use the same image for all our steps in the chain. Depending on
# the CloudEvent metadata the image will do one thing or another. In
# addition, the channel and subscription structure enforces the right routing
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: word-count-splitter
  namespace: tless
spec:
  template:
    spec:
      # runtimeClassName: kata-qemu-sev
      containers:
        - image: ghcr.io/coco-serverless/tless-knative-worker:0.2.0
          ports:
            - containerPort: 8080
          workingDir: /code/faasm-examples/workflows/word-count/knative
          command: [ "cargo", "run", "--release" ]
    metadata:
      labels:
        tless.workflows/name: word-count-splitter
      annotations:
        # NOTE: we may have to enable this annotation in Kata's config file
        # under hypervisor.qemu.enable_annotations (add 'default_memory')
        io.katacontainers.config.hypervisor.default_memory: "6144"
---
# JobSink guarantees one Job per CloudEvent, satisfying our dynamic scale-up
# requirements. However, JobSink's propagate CloudEvents through a volume
# mount, rather than an HTTP request.
apiVersion: sinks.knative.dev/v1alpha1
kind: JobSink
metadata:
  name: word-count-mapper
  namespace: tless
spec:
  job:
    spec:
      completions: 1
      parallelism: 1
      backoffLimit: 4
      template:
        spec:
          # runtimeClassName: kata-qemu-sev
          restartPolicy: Never
          containers:
            - name: main
              image: ghcr.io/coco-serverless/tless-knative-worker:0.2.0
              ports:
                - containerPort: 8080
              workingDir: /code/faasm-examples/workflows/word-count/knative
              command: [ "cargo", "run", "--release" ]
              env:
                - name: CE_FROM_FILE
                  value: "on"
        metadata:
          labels:
            tless.workflows/name: word-count-mapper
          annotations:
            # NOTE: we may have to enable this annotation in Kata's config file
            # under hypervisor.qemu.enable_annotations (add 'default_memory')
            io.katacontainers.config.hypervisor.default_memory: "6144"
---
# For this last service, we want to give it a high-grace period to make sure
# that the same instance processes all of the events
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: word-count-reducer
  namespace: tless
spec:
  template:
    spec:
      # runtimeClassName: kata-qemu-sev
      containers:
        - image: ghcr.io/coco-serverless/tless-knative-worker:0.2.0
          ports:
            - containerPort: 8080
          workingDir: /code/faasm-examples/workflows/word-count/knative
          command: [ "cargo", "run", "--release" ]
    metadata:
      labels:
        tless.workflows/name: word-count-reducer
      annotations:
        autoscaling.knative.dev/scale-to-zero-pod-retention-period: "1m"
        # NOTE: we may have to enable this annotation in Kata's config file
        # under hypervisor.qemu.enable_annotations (add 'default_memory')
        io.katacontainers.config.hypervisor.default_memory: "6144"
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-one-subscription
  namespace: tless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: ingress-to-splitter
  reply:
    ref:
      apiVersion: messaging.knative.dev/v1
      kind: InMemoryChannel
      name: splitter-to-mapper
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: word-count-splitter
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-two-subscription
  namespace: tless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: splitter-to-mapper
  subscriber:
    ref:
      apiVersion: sinks.knative.dev/v1alpha1
      kind: JobSink
      name: word-count-mapper
---
apiVersion: messaging.knative.dev/v1
kind: Subscription
metadata:
  name: edge-three-subscription
  namespace: tless
spec:
  channel:
    apiVersion: messaging.knative.dev/v1
    kind: Channel
    name: mapper-to-reducer
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: word-count-reducer
---
